{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bc298b-e6e8-4ede-bd4c-8e12fac1bc12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Desafio técnico MINEHR\n",
    "\n",
    "### Por: Katia Cardoso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4880b622-eeb9-4e6d-b184-f834a7e3c08b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importação de diferentes bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5be4d1-8d90-4dbf-9b6a-15ac0ea67d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# URL do arquivo CSV\n",
    "url_csv = 'https://storage.googleapis.com/desafio-ed/ingestion/base_colaboradores_2022.csv'\n",
    "# URL do arquivo Excel (.xlsx)\n",
    "url_excel = 'https://storage.googleapis.com/desafio-ed/ingestion/base_colaboradores_2023.xlsx'\n",
    "\n",
    "# Importando dados do Excel Online\n",
    "df_xlsx = pd.read_excel(url_excel)\n",
    "\n",
    "# Substitua todos os espaços vazios por null\n",
    "#df_xlsx.fillna(np.nan, inplace=True)\n",
    "\n",
    "# Importando dados do CSV com codificação \"latin-1\" e separador \";\"\n",
    "df_csv = pd.read_csv(url_csv, encoding='latin-1', sep=';')\n",
    "# Substitua todos os espaços vazios por null\n",
    "#df_csv.fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1703bc43-1d0e-4d21-b6af-96519fafacee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Padronização dos nomes das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1845e35-e459-4dee-bb70-adf7889f453a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas de df_csv antes dos ajustes:\nIndex(['Matrícula    ', 'MÊS REFERENCIA', 'FUNCIONARIO', 'DATA ADMISSAO',\n       'NACIONALIDADE', 'DATA DEMISSAO', 'ESTADO CIVIL', 'SEXO', 'CARGO',\n       'EMPRESA', 'DATA NASCIMENTO'],\n      dtype='object')\n\nNomes das colunas de df_excel antes dos ajustes:\nIndex(['mês referência', 'matricula', 'data admissao', 'funcionario',\n       'nacionalidade', 'estado civil', 'sexo', 'cargo', 'empresa',\n       'data de nascimento', 'data demissao'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os nomes das colunas dos DataFrames\n",
    "print(\"Nomes das colunas de df_csv antes dos ajustes:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "print(\"\\nNomes das colunas de df_excel antes dos ajustes:\")\n",
    "print(df_xlsx.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae010b8-6312-4432-b707-f046482de157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas diferentes nos DataFrames: []\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "# Função para normalizar nomes de colunas\n",
    "def normalize_column_names(df):\n",
    "    # Normalizar nomes das colunas (acentos, minúsculas e espaços)\n",
    "    normalized_columns = [unidecode(col).lower().strip().replace(' ', '_') for col in df.columns]\n",
    "    df.columns = normalized_columns\n",
    "    return df\n",
    "\n",
    "# Normalizar nomes das colunas para ambos os DataFrames\n",
    "df_csv = normalize_column_names(df_csv)\n",
    "df_xlsx = normalize_column_names(df_xlsx)\n",
    "\n",
    "# Modificar o nome da coluna \"data_de_nascimento\" no DataFrame Excel\n",
    "df_xlsx.rename(columns={'data_de_nascimento': 'data_nascimento'}, inplace=True)\n",
    "\n",
    "# Verificar se os nomes das colunas são idênticos\n",
    "if df_csv.columns.equals(df_xlsx.columns):\n",
    "    # Se os nomes das colunas são idênticos, reorganize as colunas\n",
    "    df_csv = df_csv[df_xlsx.columns]\n",
    "else:\n",
    "    # Imprimir as colunas que são diferentes\n",
    "    different_columns = [col for col in df_csv.columns if col not in df_xlsx.columns]\n",
    "    print(f\"Colunas diferentes nos DataFrames: {different_columns}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d647a463-6bb0-4b54-934a-9aee1b1ce075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas de df_csv apos ajustes:\nIndex(['matricula', 'mes_referencia', 'funcionario', 'data_admissao',\n       'nacionalidade', 'data_demissao', 'estado_civil', 'sexo', 'cargo',\n       'empresa', 'data_nascimento'],\n      dtype='object')\n\nNomes das colunas de df_excel apos ajustes:\nIndex(['mes_referencia', 'matricula', 'data_admissao', 'funcionario',\n       'nacionalidade', 'estado_civil', 'sexo', 'cargo', 'empresa',\n       'data_nascimento', 'data_demissao'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os nomes das colunas dos DataFrames\n",
    "print(\"Nomes das colunas de df_csv apos ajustes:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "print(\"\\nNomes das colunas de df_excel apos ajustes:\")\n",
    "print(df_xlsx.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdfb512-a02c-486a-b834-04eb083cb30c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## União dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8280008f-ddcb-4e60-851d-bacc7cab4ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matricula</th>\n",
       "      <th>mes_referencia</th>\n",
       "      <th>funcionario</th>\n",
       "      <th>data_admissao</th>\n",
       "      <th>nacionalidade</th>\n",
       "      <th>data_demissao</th>\n",
       "      <th>estado_civil</th>\n",
       "      <th>sexo</th>\n",
       "      <th>cargo</th>\n",
       "      <th>empresa</th>\n",
       "      <th>data_nascimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>28/02/2022</td>\n",
       "      <td>Colaborador 27</td>\n",
       "      <td>05/12/2005</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Eletr Lv Transmissao Iii</td>\n",
       "      <td>Empresa 6</td>\n",
       "      <td>01/01/1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>31/01/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 5</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>28/02/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 5</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>31/03/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 2</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>30/04/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 2</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matricula mes_referencia  ...    empresa data_nascimento\n",
       "0         27     28/02/2022  ...  Empresa 6      01/01/1977\n",
       "1         28     31/01/2022  ...  Empresa 5      01/04/1979\n",
       "2         28     28/02/2022  ...  Empresa 5      01/04/1979\n",
       "3         28     31/03/2022  ...  Empresa 2      01/04/1979\n",
       "4         28     30/04/2022  ...  Empresa 2      01/04/1979\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenar os DataFrames por linhas (um abaixo do outro)\n",
    "df_final = pd.concat([df_csv, df_xlsx], ignore_index=True)\n",
    "\n",
    "# Salvando o DataFrame como um arquivo CSV\n",
    "df_final.to_csv('df_final.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Visualizando as primeiras 5 linhas do DataFrame\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d486a60-4004-4fc7-acab-d7a8dbddc745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas csv : 20054\nNúmero de colunas csv: 11 \n\nNúmero de linhas xlsx: 27297\nNúmero de colunas xlsx : 11 \n \nNúmero de linhas final: 47351\nNúmero de colunas final: 11\n"
     ]
    }
   ],
   "source": [
    "#Verificação do numero de linhas e colunas arquivo .csv\n",
    "num_linhas_csv, num_colunas_csv = df_csv.shape\n",
    "print(f'Número de linhas csv : {num_linhas_csv}')\n",
    "print(f'Número de colunas csv: {num_colunas_csv} \\n')\n",
    "\n",
    "\n",
    "#Verificação do numero de linhas e colunas arquivo .xlsx\n",
    "num_linhas_xlsx, num_colunas_xlsx = df_xlsx.shape\n",
    "print(f'Número de linhas xlsx: {num_linhas_xlsx}')\n",
    "print(f'Número de colunas xlsx : {num_colunas_xlsx} \\n ')\n",
    "\n",
    "#Verificação do numero de linhas e colunas arquivo final\n",
    "num_linhas, num_colunas = df_final.shape\n",
    "print(f'Número de linhas final: {num_linhas}')\n",
    "print(f'Número de colunas final: {num_colunas}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1db9ad-da91-4524-8e9c-eeb4019fcc26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conversão para um DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d732de-5a21-44c3-b88b-777c7a59da2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 47351 entries, 0 to 47350\nData columns (total 11 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   matricula        47351 non-null  int64 \n 1   mes_referencia   47351 non-null  object\n 2   funcionario      47351 non-null  object\n 3   data_admissao    47351 non-null  object\n 4   nacionalidade    47346 non-null  object\n 5   data_demissao    436 non-null    object\n 6   estado_civil     47116 non-null  object\n 7   sexo             47351 non-null  object\n 8   cargo            47351 non-null  object\n 9   empresa          47351 non-null  object\n 10  data_nascimento  47351 non-null  object\ndtypes: int64(1), object(10)\nmemory usage: 4.0+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "# Verificação dos tipos de dados das colunas\n",
    "print(df_final.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fa7b57-d425-40d5-98aa-6a14d7e4b8e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "\n",
    "# Conversão do tipo de dados das colunas com o tipo 'object' para 'str'\n",
    "for col in df_final.columns:\n",
    "    if df_final[col].dtype == 'object':\n",
    "        df_final[col] = df_final[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9a24d5-1912-4d27-94eb-5174daec8147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- matricula: long (nullable = true)\n |-- mes_referencia: string (nullable = true)\n |-- funcionario: string (nullable = true)\n |-- data_admissao: string (nullable = true)\n |-- nacionalidade: string (nullable = true)\n |-- data_demissao: string (nullable = true)\n |-- estado_civil: string (nullable = true)\n |-- sexo: string (nullable = true)\n |-- cargo: string (nullable = true)\n |-- empresa: string (nullable = true)\n |-- data_nascimento: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Conversão do DataFrame do Pandas para um DataFrame do Spark\n",
    "spark_df = spark.createDataFrame(df_final)\n",
    "\n",
    "# Exiba o esquema do DataFrame Spark\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b2a5d0-717c-417a-849f-5009345964d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|               cargo|  empresa|data_nascimento|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|     brasil  |          nan|      Casado|Masculino|Eletr Lv Transmis...|Empresa 6|     01/01/1977|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|      Casado| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       brasil|          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          nan|      Casado|Masculino|Sup Operacao Usina I|Empresa 6|     04/03/1987|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          nan|      casado|Masculino|            Soldador|Empresa 6|     03/04/1984|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          nan|      casado|Masculino|Sup Projetos Obra...|Empresa 6|     28/08/1984|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 5|     06/08/1982|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Exiba o DataFrame ajustado\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec98bef-5a90-4e20-8fc6-409a4cb4ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Verificação do tipo de dados do DataFrame\n",
    "print(type(spark_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9d7d1d-8989-4c2d-ba71-1995924501a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('matricula', 'bigint'), ('mes_referencia', 'string'), ('funcionario', 'string'), ('data_admissao', 'string'), ('nacionalidade', 'string'), ('data_demissao', 'string'), ('estado_civil', 'string'), ('sexo', 'string'), ('cargo', 'string'), ('empresa', 'string'), ('data_nascimento', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# Imprime o esquema do DataFrame\n",
    "print(spark_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91665fc9-df72-4989-a930-64a89a6150e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Padronização dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32585c30-94bc-4c74-9782-dcf367ac3e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " - trim(col(coluna)) remove os espaços em branco no início e no final de cada string na coluna\n",
    " - initcap() coloca as iniciais das palavras em maiúsculas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72aaeb7-884c-44e8-b399-87d3d5f711a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|               cargo|  empresa|data_nascimento|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Eletr Lv Transmis...|Empresa 6|     01/01/1977|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Sup Operacao Usina I|Empresa 6|     04/03/1987|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|            Soldador|Empresa 6|     03/04/1984|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Sup Projetos Obra...|Empresa 6|     28/08/1984|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 5|     06/08/1982|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, initcap, col\n",
    "\n",
    "# Lista de todas as colunas do DataFrame\n",
    "columns = spark_df.columns\n",
    "\n",
    "# Lista de colunas textuais \n",
    "textual_columns = ['mes_referencia', 'funcionario', 'data_admissao', 'nacionalidade', 'data_demissao', 'estado_civil', 'sexo', 'cargo', 'empresa', 'data_nascimento']  \n",
    "\n",
    "# Aplicação das transformações nas colunas textuais\n",
    "for column in textual_columns:\n",
    "    spark_df = spark_df.withColumn(column, initcap(trim(col(column))))\n",
    "\n",
    "# Mostra o DataFrame após as transformações\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8efe7f7-de05-4898-95ec-7313b9bb56d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "verificar requisito: Para as colunas numéricas, certifique-se que tenha no máximo 4 casa decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4267cc2-ec33-40f9-8a43-234a68e78194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|comprimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "# Nome da coluna numérica \n",
    "numeric_columns = 'matricula'\n",
    "\n",
    "# Verificação do comprimento dos valores na coluna numérica\n",
    "spark_df.withColumn('comprimento', length(col(numeric_columns))).filter(col('comprimento') > 4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791a0f01-f4dd-418f-8cdd-8e52c01a07e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "abordagem de valores nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd714654-e7a8-4ef4-8572-373db1c67baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|        0|             0|          0|            0|            5|        19886|         235|   0|    0|      0|              0|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importe a função isnan do módulo pyspark.sql.functions\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Use a função isnan para verificar valores nulos em todas as colunas\n",
    "spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16be70c0-3621-4049-8e6e-2afb4629a698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coluna 'matricula' tem 0.00% de dados nulos ou NaN.\nA coluna 'mes_referencia' tem 0.00% de dados nulos ou NaN.\nA coluna 'funcionario' tem 0.00% de dados nulos ou NaN.\nA coluna 'data_admissao' tem 0.00% de dados nulos ou NaN.\nA coluna 'nacionalidade' tem 0.01% de dados nulos ou NaN.\nA coluna 'data_demissao' tem 42.00% de dados nulos ou NaN.\nA coluna 'estado_civil' tem 0.50% de dados nulos ou NaN.\nA coluna 'sexo' tem 0.00% de dados nulos ou NaN.\nA coluna 'cargo' tem 0.00% de dados nulos ou NaN.\nA coluna 'empresa' tem 0.00% de dados nulos ou NaN.\nA coluna 'data_nascimento' tem 0.00% de dados nulos ou NaN.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "# Calcula o número total de linhas no DataFrame\n",
    "total_linhas = spark_df.count()\n",
    "\n",
    "# Calcula a porcentagem de dados nulos ou NaN para cada coluna\n",
    "porcentagens_nulos_nan = []\n",
    "for coluna in spark_df.columns:\n",
    "    # Calcula o número de linhas com valores nulos ou NaN na coluna atual\n",
    "    nulos_ou_nan_na_coluna = spark_df.where((col(coluna).isNull()) | (isnan(col(coluna)))).count()\n",
    "    \n",
    "    # Calcula a porcentagem de dados nulos ou NaN na coluna atual\n",
    "    porcentagem_nulos_nan = (nulos_ou_nan_na_coluna / total_linhas) * 100\n",
    "    \n",
    "    # Adiciona o nome da coluna e a porcentagem de nulos ou NaN à lista\n",
    "    porcentagens_nulos_nan.append((coluna, porcentagem_nulos_nan))\n",
    "\n",
    "# Mostra as porcentagens de dados nulos ou NaN para todas as colunas\n",
    "for coluna, porcentagem in porcentagens_nulos_nan:\n",
    "    print(f\"A coluna '{coluna}' tem {porcentagem:.2f}% de dados nulos ou NaN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a04907-feff-4a2f-b894-b15be39f5bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "fazer comentarios após analises de quantidades de numeros nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512f5229-c6c3-4e69-9a15-50ccdde90b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'from pyspark.sql.functions import col\\n\\n# Exclui linhas com valores nulos nas colunas \"nacionalidade\" e \"estado_civil\"\\ndf_sem_nulos = spark_df.dropna(subset=[\"nacionalidade\", \"estado_civil\"])\\n\\n\\n# Use a função isnan para verificar valores nulos em todas as colunas\\ndf_sem_nulos.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()'"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from pyspark.sql.functions import col\n",
    "\n",
    "# Exclui linhas com valores nulos nas colunas \"nacionalidade\" e \"estado_civil\"\n",
    "df_sem_nulos = spark_df.dropna(subset=[\"nacionalidade\", \"estado_civil\"])\n",
    "\n",
    "\n",
    "# Use a função isnan para verificar valores nulos em todas as colunas\n",
    "df_sem_nulos.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da3df98-8949-49b2-9e40-9f877a398e68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|        0|             0|          0|            0|            5|        19886|         235|   0|    0|      0|              0|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|               cargo|  empresa|data_nascimento|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Eletr Lv Transmis...|Empresa 6|     01/01/1977|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Sup Operacao Usina I|Empresa 6|     04/03/1987|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|            Soldador|Empresa 6|     03/04/1984|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Sup Projetos Obra...|Empresa 6|     28/08/1984|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 5|     06/08/1982|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "colunas_interesse = ['nacionalidade','data_nascimento', 'estado_civil']\n",
    "\n",
    "# Remover linhas com valores nulos nas colunas especificadas\n",
    "spark_df = spark_df.dropna(how=\"all\")\n",
    "# Use a função isnan para verificar valores nulos em todas as colunas\n",
    "spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()\n",
    "\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c6a2d8-bded-4e3c-9f02-3cf976d4a616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "coluna via-para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47849961-1219-4733-b76b-415fd66646a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grupo cargo</th>\n",
       "      <th>cargo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Adm Contratos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Administrador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Administrador Tecnico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Analista Administ Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Anl Administrativo I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grupo cargo                     cargo\n",
       "0       Analista          Adm Contratos\n",
       "1       Analista          Administrador\n",
       "2       Analista  Administrador Tecnico\n",
       "3       Analista   Analista Administ Jr\n",
       "4       Analista   Anl Administrativo I"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crie um DataFrame \n",
    "\n",
    "# URL do arquivo Excel (.xlsx)\n",
    "url= 'https://storage.googleapis.com/desafio-ed/ingestion/de-para.xlsx'\n",
    "\n",
    "# Importando dados do Excel Online\n",
    "df_de_para = pd.read_excel(url)\n",
    "\n",
    "df_de_para.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06b7db7-6bd0-4619-833e-e0b65bd9691b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- grupo_cargo: string (nullable = true)\n |-- cargo: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_de_para = normalize_column_names(df_de_para)\n",
    "\n",
    "# Suponha que você tenha o DataFrame Pandas chamado \"mapeamento_df_pandas\" com as colunas \"cargo\" e \"grupo_cargo\"\n",
    "# Converta o DataFrame Pandas para um DataFrame PySpark\n",
    "spark_de_para = spark.createDataFrame(df_de_para)\n",
    "\n",
    "spark_de_para.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5e5fc1-50f2-4049-92b8-9f98142ca0f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     01/01/1977|         Tecnico|Eletr Lv Transmis...|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     04/03/1987|Supervisor/lider|Sup Operacao Usina I|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     03/04/1984|     Operacional|            Soldador|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     28/08/1984|Supervisor/lider|Sup Projetos Obra...|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Realize o broadcast do DataFrame de mapeamento para otimizar o join\n",
    "broadcast = broadcast(spark_de_para)\n",
    "\n",
    "# Realize o join entre o DataFrame original e o DataFrame de mapeamento usando a coluna \"cargo\" como chave\n",
    "spark_df = spark_df.join(broadcast, \"cargo\", \"left_outer\")\n",
    "\n",
    "# Reorganize as colunas para ter \"grupo_cargo\" à esquerda de \"Cargo\"\n",
    "spark_df = spark_df.select(*[coluna for coluna in spark_df.columns if coluna not in [\"cargo\", \"grupo_cargo\"]],\n",
    "                           \"grupo_cargo\", \"cargo\")\n",
    "\n",
    "# Mostra o DataFrame resultante\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1001f3ad-cea3-4334-8e81-867d672142e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas após a remoção de valores nulos ou NaN: 47351\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     01/01/1977|         Tecnico|Eletr Lv Transmis...|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     04/03/1987|Supervisor/lider|Sup Operacao Usina I|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     03/04/1984|     Operacional|            Soldador|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     28/08/1984|Supervisor/lider|Sup Projetos Obra...|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar o número de linhas após a remoção de valores nulos ou NaN\n",
    "num_rows = spark_df.count()\n",
    "\n",
    "from pyspark.sql.functions import isnan, col\n",
    "\n",
    "# Especificar o nome da coluna com valores nulos ou NaN\n",
    "nome_coluna = \"data_demissao\"\n",
    "\n",
    "'''# Remover linhas com valores nulos ou NaN na coluna especificada\n",
    "spark_df = spark_df.na.drop(subset=[nome_coluna])\n",
    "\n",
    "# Verificar se existem valores nulos ou NaN na coluna especificada\n",
    "if spark_df.filter((isnan(col(nome_coluna)) | col(nome_coluna).isNull())).count() == 0:\n",
    "    print(f\"Linhas com valores nulos ou NaN na coluna '{nome_coluna}' foram removidas com sucesso.\")\n",
    "else:\n",
    "    print(f\"Ainda existem linhas com valores nulos ou NaN na coluna '{nome_coluna}'.\")'''\n",
    "\n",
    "\n",
    "\n",
    "num_rows_after = spark_df.count()\n",
    "print(f\"Número de linhas após a remoção de valores nulos ou NaN: {num_rows_after}\")\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bad944-6c9f-43b2-b20e-7c56dd97a6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, months_between, when, current_date,col\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import datediff, to_date, lit, floor\n",
    "\n",
    "# Filtrando linhas com valores nulos nas colunas relevantes\n",
    "#spark_df = spark_df.filter(col(\"mes_referencia\").isNotNull() & col(\"data_nascimento\").isNotNull() & col(\"data_admissao\").isNotNull() & col(\"data_demissao\").isNotNull())\n",
    "\n",
    "# Converter as strings de data para o formato de data\n",
    "spark_df = spark_df.withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"dd/MM/yyyy\"))\n",
    "spark_df = spark_df.withColumn(\"mes_referencia\", to_date(col(\"mes_referencia\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Calcular a diferença em dias entre a data de referência e a data de nascimento\n",
    "diferenca_dias = datediff(col(\"mes_referencia\"), col(\"data_nascimento\"))\n",
    "\n",
    "# Calcular a idade em anos (dividir por 365 e arredondar para baixo)\n",
    "idade = floor(diferenca_dias / lit(365))\n",
    "\n",
    "# Adicionar a coluna de idade ao DataFrame original\n",
    "spark_df = spark_df.withColumn(\"idade\", idade)\n",
    "\n",
    "# Mostrar o DataFrame resultante\n",
    "#spark_df.show()\n",
    "\n",
    "\n",
    "# Filtrar linhas onde data_demissao é nula (ainda não foram demitidos)\n",
    "funcionarios_ativos = spark_df.filter(col(\"data_demissao\").isNull())\n",
    "\n",
    "# Converter as strings de data para o formato de data\n",
    "spark_df = spark_df.withColumn(\"data_admissao\", to_date(col(\"data_admissao\"), \"dd/MM/yyyy\"))\n",
    "spark_df = spark_df.withColumn(\"mes_referencia\", to_date(col(\"mes_referencia\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Calcular a diferença em meses entre a data de demissão e a data de admissão\n",
    "diferenca_meses = months_between(col(\"mes_referencia\"), col(\"data_admissao\"))\n",
    "\n",
    "# Arredondar para baixo o resultado da diferença em meses\n",
    "tempo_empresa_meses = floor(diferenca_meses)\n",
    "\n",
    "# Adicionar a coluna de tempo na empresa ao DataFrame original\n",
    "spark_df = spark_df.withColumn(\"tempo_empresa\", tempo_empresa_meses)\n",
    "\n",
    "# Mostra o DataFrame resultante\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5655ee24-5ebe-4113-9bc0-bfbb8fb42c69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n|      ds_idade_cat|ds_tempo_empresa_cat|\n+------------------+--------------------+\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+------------------+--------------------+\nonly showing top 20 rows\n\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|      ds_idade_cat|ds_tempo_empresa_cat|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Definir as condições para categorias de idade\n",
    "condicoes_idade = [\n",
    "    (col(\"idade\") < 21, \"Menos de 21 Anos\"),\n",
    "    (col(\"idade\").between(21, 25), \"Entre 21 E 25 Anos\"),\n",
    "    (col(\"idade\").between(26, 30), \"Entre 26 E 30 Anos\"),\n",
    "    (col(\"idade\").between(31, 40), \"Entre 31 E 40 Anos\"),\n",
    "    (col(\"idade\").between(41, 50), \"Entre 41 E 50 Anos\"),\n",
    "    (col(\"idade\") > 50, \"Acima De 50 Anos\")\n",
    "]\n",
    "\n",
    "# Definir as condições para categorias de tempo de empresa (em meses)\n",
    "condicoes_tempo_empresa = [\n",
    "    (col(\"tempo_empresa\") < 3, \"Até 3 meses\"),\n",
    "    (col(\"tempo_empresa\").between(3, 6), \"Entre 3 E 6 Meses\"),\n",
    "    (col(\"tempo_empresa\").between(2, 12), \"Entre 6 Meses E 1 Ano\"),\n",
    "    (col(\"tempo_empresa\").between(12, 24), \"Entre 1 E 2 Anos\"),\n",
    "    (col(\"tempo_empresa\").between(24, 60), \"Entre 2 E 5 Anos\"),\n",
    "    (col(\"tempo_empresa\").between(60, 120), \"Entre 5 E 10 Anos\"),\n",
    "    (col(\"tempo_empresa\") > 120, \"Acima De 10 Anos\")\n",
    "]\n",
    "\n",
    "# Aplicar as condições e criar as colunas categóricas\n",
    "spark_df = spark_df.withColumn(\"ds_idade_cat\", \n",
    "                                                   when(condicoes_idade[0][0], condicoes_idade[0][1])\n",
    "                                                    .when(condicoes_idade[1][0], condicoes_idade[1][1])\n",
    "                                                    .when(condicoes_idade[2][0], condicoes_idade[2][1])\n",
    "                                                    .when(condicoes_idade[3][0], condicoes_idade[3][1])\n",
    "                                                    .when(condicoes_idade[4][0], condicoes_idade[4][1])\n",
    "                                                    .when(condicoes_idade[5][0], condicoes_idade[5][1])\n",
    "                                                    .otherwise(\"Desconhecido\"))\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "spark_df = spark_df.withColumn(\"ds_tempo_empresa_cat\", \n",
    "                                                 when(condicoes_tempo_empresa[0][0], condicoes_tempo_empresa[0][1])\n",
    "                                                    .when(condicoes_tempo_empresa[1][0], condicoes_tempo_empresa[1][1])\n",
    "                                                    .when(condicoes_tempo_empresa[2][0], condicoes_tempo_empresa[2][1])\n",
    "                                                    .when(condicoes_tempo_empresa[3][0], condicoes_tempo_empresa[3][1])\n",
    "                                                    .when(condicoes_tempo_empresa[4][0], condicoes_tempo_empresa[4][1])\n",
    "                                                    .when(condicoes_tempo_empresa[5][0], condicoes_tempo_empresa[5][1])\n",
    "                                                    .when(condicoes_tempo_empresa[6][0], condicoes_tempo_empresa[6][1])\n",
    "                                                    .otherwise(\"Desconhecido\"))\n",
    "\n",
    "# Mostrar o DataFrame resultante com as novas colunas categóricas\n",
    "spark_df.select(\"ds_idade_cat\", \"ds_tempo_empresa_cat\").show()\n",
    "\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d0fbdb-37ad-45bb-bfbd-729af7026ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "tentar uma contagem de categoria, algo assim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca2d0b7-9420-4bd9-b99b-3e16d46d7494",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo /Workspace/Users/kt.cardosos@gmail.com/mensalizada.parquet foi excluído com sucesso!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3810723377291333>, line 19\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m local_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/Katia/Downloads/mensalizada.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Baixa o arquivo\u001B[39;00m\n",
       "\u001B[0;32m---> 19\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mcp(caminho_parquet, local_path,\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o74420.cp.\n",
       ": org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"C\"\n",
       "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
       "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
       "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
       "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.getFS(DBUtilsCore.scala:496)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$3(DBUtilsCore.scala:336)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:160)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:152)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:152)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:333)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:333)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:332)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3810723377291333>, line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m local_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/Katia/Downloads/mensalizada.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Baixa o arquivo\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mcp(caminho_parquet, local_path,\u001B[38;5;28;01mTrue\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o74420.cp.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"C\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.getFS(DBUtilsCore.scala:496)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$3(DBUtilsCore.scala:336)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$2(DBUtilsCore.scala:160)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$withCpSafetyChecks$1(DBUtilsCore.scala:152)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withCpSafetyChecks(DBUtilsCore.scala:152)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$2(DBUtilsCore.scala:333)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$cp$1(DBUtilsCore.scala:333)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala:332)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"C\"",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Especifique o caminho onde você deseja salvar o arquivo Parquet\n",
    "caminho_parquet = \"/Workspace/Users/kt.cardosos@gmail.com/mensalizada.parquet\"\n",
    "# Salve o DataFrame como um arquivo Parquet\n",
    "spark_df.write.mode(\"overwrite\").parquet(caminho_parquet)\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "if dbutils.fs.rm(caminho_parquet, True):\n",
    "    print(f\"Arquivo {caminho_parquet} foi excluído com sucesso!\")\n",
    "else:\n",
    "    print(f\"O arquivo {caminho_parquet} não existe ou não pode ser excluído.\")\n",
    "\n",
    "# Em seguida, escreve o novo arquivo Parquet\n",
    "spark_df.write.parquet(caminho_parquet)\n",
    "\n",
    "# Caminho para o local onde você deseja salvar o arquivo no seu computador local\n",
    "local_path = \"C:/Users/Katia/Downloads/mensalizada.parquet\"\n",
    "\n",
    "# Baixa o arquivo\n",
    "dbutils.fs.cp(caminho_parquet, local_path,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b57881-81da-4529-a24d-b378d25d90be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realiza o join entre o DataFrame original e o DataFrame com as categorias usando a coluna 'id_funcionario' como chave\n",
    "#spark_df = spark_df.join(, 'matricula', 'left_outer')\n",
    "\n",
    "# Mostra o DataFrame resultante\n",
    "#spark_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Teste_Tecnico_MINEHR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
