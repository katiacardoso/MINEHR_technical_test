{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bc298b-e6e8-4ede-bd4c-8e12fac1bc12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Desafio técnico MINEHR\n",
    "\n",
    "### Por: Katia Cardoso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4880b622-eeb9-4e6d-b184-f834a7e3c08b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importação de diferentes bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5be4d1-8d90-4dbf-9b6a-15ac0ea67d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# URL do arquivo CSV\n",
    "url_csv = 'https://storage.googleapis.com/desafio-ed/ingestion/base_colaboradores_2022.csv'\n",
    "# URL do arquivo Excel (.xlsx)\n",
    "url_excel = 'https://storage.googleapis.com/desafio-ed/ingestion/base_colaboradores_2023.xlsx'\n",
    "\n",
    "# Importando dados do Excel Online\n",
    "df_xlsx = pd.read_excel(url_excel)\n",
    "\n",
    "# Importando dados do CSV com codificação \"latin-1\" e separador \";\"\n",
    "df_csv = pd.read_csv(url_csv, encoding='latin-1', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1703bc43-1d0e-4d21-b6af-96519fafacee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Padronização dos nomes das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce5bd21-4b48-4a88-b285-7dd0173fe08b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Visualização do nome das colunas de cada DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1845e35-e459-4dee-bb70-adf7889f453a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas de df_csv antes dos ajustes:\nIndex(['Matrícula    ', 'MÊS REFERENCIA', 'FUNCIONARIO', 'DATA ADMISSAO',\n       'NACIONALIDADE', 'DATA DEMISSAO', 'ESTADO CIVIL', 'SEXO', 'CARGO',\n       'EMPRESA', 'DATA NASCIMENTO'],\n      dtype='object')\n\nNomes das colunas de df_xlsx antes dos ajustes:\nIndex(['mês referência', 'matricula', 'data admissao', 'funcionario',\n       'nacionalidade', 'estado civil', 'sexo', 'cargo', 'empresa',\n       'data de nascimento', 'data demissao'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os nomes das colunas dos DataFrames\n",
    "print(\"Nomes das colunas de df_csv antes dos ajustes:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "print(\"\\nNomes das colunas de df_xlsx antes dos ajustes:\")\n",
    "print(df_xlsx.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a872cf58-61e9-4fce-9826-d7084a152441",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Realização da padronização dos nomes das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae010b8-6312-4432-b707-f046482de157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas diferentes nos DataFrames: []\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "# Função para padronizar nomes de colunas\n",
    "def normalize_column_names(df):\n",
    "    # Padronizar nomes das colunas (acentos, minúsculas e espaços)\n",
    "    normalized_columns = [unidecode(col).lower().strip().replace(' ', '_') for col in df.columns]\n",
    "    df.columns = normalized_columns\n",
    "    return df\n",
    "\n",
    "# Padronizar nomes das colunas para ambos os DataFrames\n",
    "df_csv = normalize_column_names(df_csv)\n",
    "df_xlsx = normalize_column_names(df_xlsx)\n",
    "\n",
    "# Modificar o nome da coluna \"data_de_nascimento\" no DataFrame do Excel\n",
    "df_xlsx.rename(columns={'data_de_nascimento': 'data_nascimento'}, inplace=True)\n",
    "\n",
    "# Verificar se os nomes das colunas são idênticos\n",
    "if df_csv.columns.equals(df_xlsx.columns):\n",
    "    # Se os nomes das colunas são idênticos, reorganize as colunas\n",
    "    df_csv = df_csv[df_xlsx.columns]\n",
    "else:\n",
    "    # Imprimir as colunas que são diferentes\n",
    "    different_columns = [col for col in df_csv.columns if col not in df_xlsx.columns]\n",
    "    print(f\"Colunas diferentes nos DataFrames: {different_columns}\")\n",
    "\n",
    "# se o resultado for [], isto quer dizer que nao tem nenhuma coluna com nome diferente. Caso contrário, irá aparecer o nome da coluna que ainda necessite de algum processo de padronização \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f36563-1fa2-4cee-b97d-1ec24a123f48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Visualização dos nomes das colunas após a padronização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d647a463-6bb0-4b54-934a-9aee1b1ce075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas de df_csv apos ajustes:\nIndex(['matricula', 'mes_referencia', 'funcionario', 'data_admissao',\n       'nacionalidade', 'data_demissao', 'estado_civil', 'sexo', 'cargo',\n       'empresa', 'data_nascimento'],\n      dtype='object')\n\nNomes das colunas de df_xlsx apos ajustes:\nIndex(['mes_referencia', 'matricula', 'data_admissao', 'funcionario',\n       'nacionalidade', 'estado_civil', 'sexo', 'cargo', 'empresa',\n       'data_nascimento', 'data_demissao'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os nomes das colunas dos DataFrames\n",
    "print(\"Nomes das colunas de df_csv apos ajustes:\")\n",
    "print(df_csv.columns)\n",
    "\n",
    "print(\"\\nNomes das colunas de df_xlsx apos ajustes:\")\n",
    "print(df_xlsx.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a9e0c0-ace3-45e3-9b51-7a59fc921547",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "É importante checar se todas as colunas estão com nomes idênticos, pois caso contrário, no momento da união, serão geradas colunas a mais "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdfb512-a02c-486a-b834-04eb083cb30c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## União das bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8280008f-ddcb-4e60-851d-bacc7cab4ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matricula</th>\n",
       "      <th>mes_referencia</th>\n",
       "      <th>funcionario</th>\n",
       "      <th>data_admissao</th>\n",
       "      <th>nacionalidade</th>\n",
       "      <th>data_demissao</th>\n",
       "      <th>estado_civil</th>\n",
       "      <th>sexo</th>\n",
       "      <th>cargo</th>\n",
       "      <th>empresa</th>\n",
       "      <th>data_nascimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>28/02/2022</td>\n",
       "      <td>Colaborador 27</td>\n",
       "      <td>05/12/2005</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Eletr Lv Transmissao Iii</td>\n",
       "      <td>Empresa 6</td>\n",
       "      <td>01/01/1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>31/01/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 5</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>28/02/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casado</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 5</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>31/03/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 2</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>30/04/2022</td>\n",
       "      <td>Colaborador 28</td>\n",
       "      <td>14/04/2003</td>\n",
       "      <td>brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Solteiro</td>\n",
       "      <td>Feminino</td>\n",
       "      <td>Coord Tesouraria</td>\n",
       "      <td>Empresa 2</td>\n",
       "      <td>01/04/1979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matricula mes_referencia  ...    empresa data_nascimento\n",
       "0         27     28/02/2022  ...  Empresa 6      01/01/1977\n",
       "1         28     31/01/2022  ...  Empresa 5      01/04/1979\n",
       "2         28     28/02/2022  ...  Empresa 5      01/04/1979\n",
       "3         28     31/03/2022  ...  Empresa 2      01/04/1979\n",
       "4         28     30/04/2022  ...  Empresa 2      01/04/1979\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenar os DataFrames por linhas (um abaixo do outro)\n",
    "df_final = pd.concat([df_csv, df_xlsx], ignore_index=True)\n",
    "\n",
    "# Salvando o DataFrame como um arquivo CSV\n",
    "df_final.to_csv('df_final.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Visualizando as primeiras 5 linhas do DataFrame\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d486a60-4004-4fc7-acab-d7a8dbddc745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas csv : 20054\nNúmero de colunas csv: 11 \n\nNúmero de linhas xlsx: 27297\nNúmero de colunas xlsx : 11 \n \nNúmero de linhas final: 47351\nNúmero de colunas final: 11\n"
     ]
    }
   ],
   "source": [
    "#Verificação do numero de linhas e colunas arquivo .csv\n",
    "num_linhas_csv, num_colunas_csv = df_csv.shape\n",
    "print(f'Número de linhas csv : {num_linhas_csv}')\n",
    "print(f'Número de colunas csv: {num_colunas_csv} \\n')\n",
    "\n",
    "\n",
    "#Verificação do numero de linhas e colunas arquivo .xlsx\n",
    "num_linhas_xlsx, num_colunas_xlsx = df_xlsx.shape\n",
    "print(f'Número de linhas xlsx: {num_linhas_xlsx}')\n",
    "print(f'Número de colunas xlsx : {num_colunas_xlsx} \\n ')\n",
    "\n",
    "#Verificação do numero de linhas e colunas arquivo final\n",
    "num_linhas, num_colunas = df_final.shape\n",
    "print(f'Número de linhas final: {num_linhas}')\n",
    "print(f'Número de colunas final: {num_colunas}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76be283-a328-4132-b52d-fe2c1905e6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Com esta última célula de verificação de contagem de linhas e colunas até o momento é possível notar que neste caso a união teve êxito, pois a soma das linhas de ambos os DataFrames separados é compativel com a quantidade do DataFrame resultante da junção dos dois. Bem como, é possível notar que a quantidade de colunas foi mantida, o que indica que não houve percas nem acréscimo de colunas no decorrer do processo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1db9ad-da91-4524-8e9c-eeb4019fcc26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conversão para um DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d732de-5a21-44c3-b88b-777c7a59da2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 47351 entries, 0 to 47350\nData columns (total 11 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   matricula        47351 non-null  int64 \n 1   mes_referencia   47351 non-null  object\n 2   funcionario      47351 non-null  object\n 3   data_admissao    47351 non-null  object\n 4   nacionalidade    47346 non-null  object\n 5   data_demissao    436 non-null    object\n 6   estado_civil     47116 non-null  object\n 7   sexo             47351 non-null  object\n 8   cargo            47351 non-null  object\n 9   empresa          47351 non-null  object\n 10  data_nascimento  47351 non-null  object\ndtypes: int64(1), object(10)\nmemory usage: 4.0+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "# Verificação dos tipos de dados das colunas\n",
    "print(df_final.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f431ce-4dac-4e2c-8f9a-1a1037cd1952",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Esta conversão abaixo das colunas com o tipo 'object' para 'str' foi necessária para garantir que todas as entradas sejam tratadas como strings. Pois o DataFrame Spark, tem um sistema de tipos mais estruturado e requer tipos de dados consistentes em cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fa7b57-d425-40d5-98aa-6a14d7e4b8e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "\n",
    "# Conversão do tipo de dados das colunas com o tipo 'object' para 'str'\n",
    "for col in df_final.columns:\n",
    "    if df_final[col].dtype == 'object':\n",
    "        df_final[col] = df_final[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9a24d5-1912-4d27-94eb-5174daec8147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- matricula: long (nullable = true)\n |-- mes_referencia: string (nullable = true)\n |-- funcionario: string (nullable = true)\n |-- data_admissao: string (nullable = true)\n |-- nacionalidade: string (nullable = true)\n |-- data_demissao: string (nullable = true)\n |-- estado_civil: string (nullable = true)\n |-- sexo: string (nullable = true)\n |-- cargo: string (nullable = true)\n |-- empresa: string (nullable = true)\n |-- data_nascimento: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Conversão do DataFrame do Pandas para um DataFrame do Spark\n",
    "spark_df = spark.createDataFrame(df_final)\n",
    "\n",
    "# Exiba o esquema do DataFrame Spark\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b2a5d0-717c-417a-849f-5009345964d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|               cargo|  empresa|data_nascimento|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|     brasil  |          nan|      Casado|Masculino|Eletr Lv Transmis...|Empresa 6|     01/01/1977|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|      Casado| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|     brasil  |          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       brasil|          nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          nan|      Casado|Masculino|Sup Operacao Usina I|Empresa 6|     04/03/1987|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          nan|      casado|Masculino|            Soldador|Empresa 6|     03/04/1984|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          nan|      casado|Masculino|Sup Projetos Obra...|Empresa 6|     28/08/1984|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 5|     06/08/1982|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          nan|      casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Exiba o DataFrame Spark\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec98bef-5a90-4e20-8fc6-409a4cb4ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Verificação do tipo de dados do DataFrame do Spark\n",
    "print(type(spark_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9d7d1d-8989-4c2d-ba71-1995924501a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('matricula', 'bigint'), ('mes_referencia', 'string'), ('funcionario', 'string'), ('data_admissao', 'string'), ('nacionalidade', 'string'), ('data_demissao', 'string'), ('estado_civil', 'string'), ('sexo', 'string'), ('cargo', 'string'), ('empresa', 'string'), ('data_nascimento', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# Imprime o esquema do DataFrame do Spark\n",
    "print(spark_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e837fae3-4251-4f3f-b328-b6c395afb08c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "verificação do número de linhas e colunas após a conversão para Dataframe do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f7b813-4ef3-4fa5-88ce-5c95b63a7f0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas: 47351\nNúmero de colunas: 11\n"
     ]
    }
   ],
   "source": [
    "# Número de linhas\n",
    "num_linhas = spark_df.count()\n",
    "\n",
    "# Número de colunas\n",
    "num_colunas = len(spark_df.columns)\n",
    "\n",
    "# Exibir o número de linhas e colunas\n",
    "print(f\"Número de linhas: {num_linhas}\")\n",
    "print(f\"Número de colunas: {num_colunas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91665fc9-df72-4989-a930-64a89a6150e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Padronização dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32585c30-94bc-4c74-9782-dcf367ac3e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Para as colunas textuais, retire os leading and trailing whitespace e coloque as iniciais maiúsculas.\n",
    " \n",
    " Resumo de algumas funções importadas:\n",
    " - trim(col(coluna)) remove os espaços em branco no início e no final de cada string na coluna\n",
    " - initcap() coloca as iniciais das palavras em maiúsculas. \n",
    "\n",
    "Como adicional, utilizei a função translate para retirada de acentuação também. Mais para frente para contagem da coluna \"estado_civil\" estava ocorrendo duplicidade dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72aaeb7-884c-44e8-b399-87d3d5f711a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|               cargo|  empresa|data_nascimento|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Eletr Lv Transmis...|Empresa 6|     01/01/1977|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|    Coord Tesouraria|Empresa 5|     01/04/1979|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|    Coord Tesouraria|Empresa 2|     01/04/1979|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Sup Operacao Usina I|Empresa 6|     04/03/1987|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|            Soldador|Empresa 6|     03/04/1984|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Sup Projetos Obra...|Empresa 6|     28/08/1984|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 5|     06/08/1982|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Ger Compras Mater...|Empresa 2|     06/08/1982|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+--------------------+---------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, initcap, col, translate\n",
    "\n",
    "# Lista de todas as colunas do DataFrame\n",
    "columns = spark_df.columns\n",
    "\n",
    "# Lista de colunas textuais \n",
    "textual_columns = ['mes_referencia', 'funcionario', 'data_admissao', 'nacionalidade', 'data_demissao', 'estado_civil', 'sexo', 'cargo', 'empresa', 'data_nascimento']  \n",
    "\n",
    "# Aplicação das transformações nas colunas textuais\n",
    "for column in textual_columns:\n",
    "    spark_df = spark_df.withColumn(column, initcap(trim(translate(col(column), 'áéíóúãõâêîôûàèìòùäëïöü', 'aeiouaoaeiouaeiou'))))\n",
    "\n",
    "# Mostra o DataFrame após as transformações\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8efe7f7-de05-4898-95ec-7313b9bb56d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Para as colunas numéricas, certifique-se que tenha no máximo 4 casa decimais. \n",
    "\n",
    "Neste código abaixo, é uma verificação de comprimento da coluna \"matricula\" única coluna numérica até o momento e aqui foi levado em consideração 4 \"digitos\" e não \"casas decimais\". Contudo, adiante terão algumas outras colunas numéricas com virgulas (media_idade) e será realizado um round (arrendondamento) para 4 cadas decimais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4267cc2-ec33-40f9-8a43-234a68e78194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|comprimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "# Nome da coluna numérica \n",
    "numeric_columns = 'matricula'\n",
    "\n",
    "# Verificação do comprimento dos valores na coluna numérica\n",
    "spark_df.withColumn('comprimento', length(col(numeric_columns))).filter(col('comprimento') > 4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791a0f01-f4dd-418f-8cdd-8e52c01a07e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Abordagem de valores nulos escolhida\n",
    "\n",
    "Para dados textuais: Substituição dos valores nulos e NaN por \"Nao preenchido\" nas colunas estado_civil e nacionalidade\n",
    "\n",
    "Para dados numéricos: Não mexer com os nulos e deixar como estão, pois se estes dados forem excluidos agora podem tendenciar analises futuras e caso eles não sejam importantes lá, é recomendado a realização de uma filtragem por dados não nulos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdaff8a-265c-40a2-9165-a3e98ff9cbc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Contagem de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd714654-e7a8-4ef4-8572-373db1c67baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|        0|             0|          0|            0|            5|        19886|         235|   0|    0|      0|              0|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "# Verificação de valores nulos em todas as colunas\n",
    "spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad03976-1271-4696-b9be-53fe09c2c7cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Substituição dos valores textuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12509c7e-c86e-4d02-93e1-a68b1c5842dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|matricula|mes_referencia|funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|sexo|cargo|empresa|data_nascimento|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n|        0|             0|          0|            0|            0|        19886|           0|   0|    0|      0|              0|\n+---------+--------------+-----------+-------------+-------------+-------------+------------+----+-----+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Substituição dos valores nulos e NaN por \"Nao preenchido\" nas colunas estado_civil e nacionalidade\n",
    "spark_df = spark_df.withColumn(\"estado_civil\", when(col(\"estado_civil\").isNull() | isnan(col(\"estado_civil\")), \"Nao preenchido\").otherwise(col(\"estado_civil\")))\n",
    "spark_df = spark_df.withColumn(\"nacionalidade\", when(col(\"nacionalidade\").isNull() | isnan(col(\"nacionalidade\")), \"Nao preenchido\").otherwise(col(\"nacionalidade\")))\n",
    "\n",
    "# Verificação de valores nulos em todas as colunas\n",
    "spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d269701f-f26d-4199-b2cd-1ed3f090780d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- verificação da porcentagem de nulos no DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16be70c0-3621-4049-8e6e-2afb4629a698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coluna 'matricula' tem 0.00% de dados nulos ou NaN.\nA coluna 'mes_referencia' tem 0.00% de dados nulos ou NaN.\nA coluna 'funcionario' tem 0.00% de dados nulos ou NaN.\nA coluna 'data_admissao' tem 0.00% de dados nulos ou NaN.\nA coluna 'nacionalidade' tem 0.00% de dados nulos ou NaN.\nA coluna 'data_demissao' tem 42.00% de dados nulos ou NaN.\nA coluna 'estado_civil' tem 0.00% de dados nulos ou NaN.\nA coluna 'sexo' tem 0.00% de dados nulos ou NaN.\nA coluna 'cargo' tem 0.00% de dados nulos ou NaN.\nA coluna 'empresa' tem 0.00% de dados nulos ou NaN.\nA coluna 'data_nascimento' tem 0.00% de dados nulos ou NaN.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "# Calcula o número total de linhas no DataFrame\n",
    "total_linhas = spark_df.count()\n",
    "\n",
    "# Calculo da porcentagem de dados nulos ou NaN para cada coluna\n",
    "porcentagens_nulos_nan = []\n",
    "\n",
    "for coluna in spark_df.columns:\n",
    "    # Calculo do número de linhas com valores nulos ou NaN na coluna atual\n",
    "    nulos_ou_nan_na_coluna = spark_df.where((col(coluna).isNull()) | (isnan(col(coluna)))).count()\n",
    "    \n",
    "    # Calculo da porcentagem de dados nulos ou NaN na coluna atual\n",
    "    porcentagem_nulos_nan = (nulos_ou_nan_na_coluna / total_linhas) * 100\n",
    "    \n",
    "    # Adiciona o nome da coluna e a porcentagem de nulos ou NaN à lista\n",
    "    porcentagens_nulos_nan.append((coluna, porcentagem_nulos_nan))\n",
    "\n",
    "# Mostra as porcentagens de dados nulos ou NaN para todas as colunas\n",
    "for coluna, porcentagem in porcentagens_nulos_nan:\n",
    "    print(f\"A coluna '{coluna}' tem {porcentagem:.2f}% de dados nulos ou NaN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a04907-feff-4a2f-b894-b15be39f5bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Após a visualização das porcentagens de números nulos de cada coluna é possível observar que a coluna \"data_demissão\" possui uma quantidade bastante expresiva de dados nulos, o que indica apenas por esta analise que um pouco mais da metade dos funcionarios presentes nesta base estão empregados na empresa e apartir dai, abrir margens para  diversas técnicas e abordagens para análise desses dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c6a2d8-bded-4e3c-9f02-3cf976d4a616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Criação de coluna via de-para (grupo cargo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab324e0-8c75-4b1c-84f6-78cbb3ac09cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Importação da base via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47849961-1219-4733-b76b-415fd66646a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grupo cargo</th>\n",
       "      <th>cargo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Adm Contratos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Administrador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Administrador Tecnico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Analista Administ Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analista</td>\n",
       "      <td>Anl Administrativo I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grupo cargo                     cargo\n",
       "0       Analista          Adm Contratos\n",
       "1       Analista          Administrador\n",
       "2       Analista  Administrador Tecnico\n",
       "3       Analista   Analista Administ Jr\n",
       "4       Analista   Anl Administrativo I"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL do arquivo Excel (.xlsx)\n",
    "url= 'https://storage.googleapis.com/desafio-ed/ingestion/de-para.xlsx'\n",
    "\n",
    "# Importação dos dados do Excel Online\n",
    "df_de_para = pd.read_excel(url)\n",
    "\n",
    "# Visualzação do DataFrame\n",
    "df_de_para.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1222f7-e3ca-41a5-af88-bb51e053faaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Padronização dos nomes das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06b7db7-6bd0-4619-833e-e0b65bd9691b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- grupo_cargo: string (nullable = true)\n |-- cargo: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Padronização do nome das colunas\n",
    "df_de_para = normalize_column_names(df_de_para)\n",
    "\n",
    "# Converção do DataFrame Pandas para um DataFrame PySpark\n",
    "spark_de_para = spark.createDataFrame(df_de_para)\n",
    "\n",
    "#Visualização do Dataframe Spark \n",
    "spark_de_para.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89efb98f-b02c-430c-ac15-c3cb55fe7f90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "União da base de_para com o DataFrame original, utilizando a função de broadcast (envia uma cópia local dessa base de_para para os clusters para quando for utilizar no join ficar mais perto do que o contato com um servidor central)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5e5fc1-50f2-4049-92b8-9f98142ca0f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\n|       27|    28/02/2022|Colaborador 27|   05/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     01/01/1977|         Tecnico|Eletr Lv Transmis...|\n|       28|    31/01/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    28/02/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/03/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/04/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/05/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/06/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/07/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/08/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/09/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/10/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    30/11/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       28|    31/12/2022|Colaborador 28|   14/04/2003|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     01/04/1979|     Coordenador|    Coord Tesouraria|\n|       31|    28/02/2022|Colaborador 31|   19/12/2005|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     04/03/1987|Supervisor/lider|Sup Operacao Usina I|\n|       34|    28/02/2022|Colaborador 34|   03/01/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     03/04/1984|     Operacional|            Soldador|\n|       44|    28/02/2022|Colaborador 44|   20/03/2006|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     28/08/1984|Supervisor/lider|Sup Projetos Obra...|\n|       53|    31/01/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    28/02/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    31/03/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n|       53|    30/04/2022|Colaborador 53|   10/06/2008|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     06/08/1982|         Gerente|Ger Compras Mater...|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Realização do broadcast do DataFrame de_para\n",
    "broadcast = broadcast(spark_de_para)\n",
    "\n",
    "# Realização do join entre o DataFrame original e o DataFrame de_para utilizando a coluna \"cargo\" como chave \n",
    "spark_df = spark_df.join(broadcast, \"cargo\", \"left_outer\")\n",
    "\n",
    "# Reorganização das colunas para ter \"grupo_cargo\" à esquerda de \"Cargo\"\n",
    "spark_df = spark_df.select(*[coluna for coluna in spark_df.columns if coluna not in [\"cargo\", \"grupo_cargo\"]], \"grupo_cargo\", \"cargo\")\n",
    "\n",
    "# Mostra o DataFrame resultante\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c3594e-8b7e-4b73-bcf7-f26face2c187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Criação das colunas inteiras: idade e tempo_empresa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e4a4d9-67b2-48e7-9b48-cf45bb7206d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- idade: idade da pessoa naquele mês referência;\n",
    "- tempo_empresa: o tempo, em meses, em que a pessoa pessoa está ativa na empresa até aquele mês;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bad944-6c9f-43b2-b20e-7c56dd97a6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, months_between, when, current_date\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import datediff, to_date, lit, floor\n",
    "\n",
    "# Conversão das strings de data para o formato de data internacional\n",
    "spark_df = spark_df.withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"dd/MM/yyyy\"))\n",
    "spark_df = spark_df.withColumn(\"mes_referencia\", to_date(col(\"mes_referencia\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Calculo da diferença em dias entre a data de referência e a data de nascimento\n",
    "diferenca_dias = datediff(col(\"mes_referencia\"), col(\"data_nascimento\"))\n",
    "\n",
    "# Calculo da idade em anos, com a divisão por 365 e arredondamento para baixo\n",
    "idade = floor(diferenca_dias / lit(365))\n",
    "\n",
    "# Adição da coluna de idade ao DataFrame original\n",
    "spark_df = spark_df.withColumn(\"idade\", idade)\n",
    "\n",
    "# Mostrar o DataFrame resultante\n",
    "#spark_df.show()\n",
    "\n",
    "\n",
    "# Filtro das linhas onde data_demissao é nula (ainda não foram demitidos)\n",
    "funcionarios_ativos = spark_df.filter(col(\"data_demissao\").isNull())\n",
    "\n",
    "# Conversão das strings de data para o formato de data internacional\n",
    "spark_df = spark_df.withColumn(\"data_admissao\", to_date(col(\"data_admissao\"), \"dd/MM/yyyy\"))\n",
    "spark_df = spark_df.withColumn(\"mes_referencia\", to_date(col(\"mes_referencia\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Calculo da diferença em meses entre a data de demissão e a data de admissão\n",
    "diferenca_meses = months_between(col(\"mes_referencia\"), col(\"data_admissao\"))\n",
    "\n",
    "# Arredondamento para baixo do resultado da diferença em meses\n",
    "tempo_empresa_meses = floor(diferenca_meses)\n",
    "\n",
    "# Adição da coluna de tempo na empresa ao DataFrame original\n",
    "spark_df = spark_df.withColumn(\"tempo_empresa\", tempo_empresa_meses)\n",
    "\n",
    "# Mostra o DataFrame resultante\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb527044-213f-4e47-be85-1c1a26687763",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Criação de duas colunas categóricas: ds_idade_cat e ds_tempo_empresa_cat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5655ee24-5ebe-4113-9bc0-bfbb8fb42c69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n|      ds_idade_cat|ds_tempo_empresa_cat|\n+------------------+--------------------+\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+------------------+--------------------+\nonly showing top 20 rows\n\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|      ds_idade_cat|ds_tempo_empresa_cat|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Definição das condições para categorias de idade\n",
    "condicoes_idade = [\n",
    "    (col(\"idade\") < 21, \"Menos de 21 Anos\"),\n",
    "    (col(\"idade\").between(21, 25), \"Entre 21 E 25 Anos\"),\n",
    "    (col(\"idade\").between(26, 30), \"Entre 26 E 30 Anos\"),\n",
    "    (col(\"idade\").between(31, 40), \"Entre 31 E 40 Anos\"),\n",
    "    (col(\"idade\").between(41, 50), \"Entre 41 E 50 Anos\"),\n",
    "    (col(\"idade\") > 50, \"Acima De 50 Anos\")\n",
    "]\n",
    "\n",
    "# Definição das condições para categorias de tempo de empresa (em meses)\n",
    "condicoes_tempo_empresa = [\n",
    "    (col(\"tempo_empresa\") < 3, \"Até 3 meses\"),\n",
    "    (col(\"tempo_empresa\").between(3, 6), \"Entre 3 E 6 Meses\"),\n",
    "    (col(\"tempo_empresa\").between(2, 12), \"Entre 6 Meses E 1 Ano\"),\n",
    "    (col(\"tempo_empresa\").between(12, 24), \"Entre 1 E 2 Anos\"),\n",
    "    (col(\"tempo_empresa\").between(24, 60), \"Entre 2 E 5 Anos\"),\n",
    "    (col(\"tempo_empresa\").between(60, 120), \"Entre 5 E 10 Anos\"),\n",
    "    (col(\"tempo_empresa\") > 120, \"Acima De 10 Anos\")\n",
    "]\n",
    "\n",
    "# Aplicação das condições e criação das colunas categóricas\n",
    "spark_df = spark_df.withColumn(\"ds_idade_cat\", \n",
    "                                                   when(condicoes_idade[0][0], condicoes_idade[0][1])\n",
    "                                                    .when(condicoes_idade[1][0], condicoes_idade[1][1])\n",
    "                                                    .when(condicoes_idade[2][0], condicoes_idade[2][1])\n",
    "                                                    .when(condicoes_idade[3][0], condicoes_idade[3][1])\n",
    "                                                    .when(condicoes_idade[4][0], condicoes_idade[4][1])\n",
    "                                                    .when(condicoes_idade[5][0], condicoes_idade[5][1])\n",
    "                                                    .otherwise(\"Desconhecido\"))\n",
    "                                            \n",
    "                                            \n",
    "                                            \n",
    "spark_df = spark_df.withColumn(\"ds_tempo_empresa_cat\", \n",
    "                                                 when(condicoes_tempo_empresa[0][0], condicoes_tempo_empresa[0][1])\n",
    "                                                    .when(condicoes_tempo_empresa[1][0], condicoes_tempo_empresa[1][1])\n",
    "                                                    .when(condicoes_tempo_empresa[2][0], condicoes_tempo_empresa[2][1])\n",
    "                                                    .when(condicoes_tempo_empresa[3][0], condicoes_tempo_empresa[3][1])\n",
    "                                                    .when(condicoes_tempo_empresa[4][0], condicoes_tempo_empresa[4][1])\n",
    "                                                    .when(condicoes_tempo_empresa[5][0], condicoes_tempo_empresa[5][1])\n",
    "                                                    .when(condicoes_tempo_empresa[6][0], condicoes_tempo_empresa[6][1])\n",
    "                                                    .otherwise(\"Desconhecido\"))\n",
    "\n",
    "# Visualizar o DataFrame resultante com as novas colunas categóricas\n",
    "spark_df.select(\"ds_idade_cat\", \"ds_tempo_empresa_cat\").show()\n",
    "\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3b9b5e-3c49-44a7-b6e6-a1b01933269a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Salvar o dataFrame neste ponto para o arquivo \"mensalizada.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a8f8fd-beb7-499b-a02b-d2cc904ab720",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Apesar de muito esforço, encontrei algumas dificuldades técnicas que me impediu de entregar o arquivo no formato parquet solicitado. As dificuldades encontradas envolveram principalmente a forma como esta plataforma trata seus arquivos e como retirar eles dela :) . Apesar desse impedimento, continuei a exploração e transformação dos dados conforme as especificações do case.\n",
    "\n",
    "Abaixo, até a proxima seção de agrupamento dos dados, seguem as minhas tentativas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47de7e12-b409-4126-8f09-5aeacb571a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark_sql_utils import to_pandas\n",
    "\n",
    "# Salva o dataframe em formato Parquet\n",
    "#spark_df.write.parquet(\"mensalizada.parquet\")\n",
    "\n",
    "# Baixa o arquivo Parquet\n",
    "#spark_df.download(\"/tmp/mensalizada.parquet\")\n",
    "#download(spark_df, \"/tmp/mensalizada.parquet\", format=\"parquet\")\n",
    "\n",
    "\n",
    "# Converte o dataframe Spark para um DataFrame Pandas\n",
    "#df_pandas = to_pandas(spark_df)\n",
    "# Salva o DataFrame Pandas em um arquivo Parquet\n",
    "#df_pandas.to_parquet(\"mensalizada.parquet\")\n",
    "\n",
    "# Salva o dataframe Spark em um arquivo Parquet\n",
    "spark_df.write.mode(\"append\").parquet(\"/tmp/mensalizada.parquet\")     \n",
    "\n",
    "#ultima tentativa que deu +-, porém o arquivo fica com tamanho zerado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43822c8-8ea9-40b2-854a-64c4a5019a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/arquivo.parquet/</td><td>arquivo.parquet/</td><td>0</td><td>1698860782664</td></tr><tr><td>dbfs:/tmp/mensalizada.parquet/</td><td>mensalizada.parquet/</td><td>0</td><td>1698860782664</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/tmp/arquivo.parquet/",
         "arquivo.parquet/",
         0,
         1698860782664
        ],
        [
         "dbfs:/tmp/mensalizada.parquet/",
         "mensalizada.parquet/",
         0,
         1698860782664
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d162c8-c57c-4f6c-9b54-e00cd6163b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls(\"dbfs:/tmp/\"))\n",
    "# Lê o dataframe Spark de um arquivo Parquet\n",
    "#df = spark_df.read.parquet(\"/tmp/arquivo.parquet\")\n",
    "\n",
    "# Mostra o dataframe Spark\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c4c290-dfa7-4e37-8b08-79b79dd7b5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls(\"dbfs:/tmp/\"))\n",
    "\n",
    "# Especifica o caminho onde deseja salvar o arquivo Parquet\n",
    "#caminho_parquet = \"/dbfs/FileStore\"\n",
    "\n",
    "# Salva o DataFrame como um arquivo Parquet\n",
    "#spark_df.write.mode(\"overwrite\").parquet(caminho_parquet)\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "#if dbutils.fs.rm(caminho_parquet, True):\n",
    "#    print(f\"Arquivo {caminho_parquet} foi excluído com sucesso!\")\n",
    "#else:\n",
    "#    print(f\"O arquivo {caminho_parquet} não existe ou não pode ser excluído.\")\n",
    "\n",
    "# Escreve o novo arquivo Parquet\n",
    "#spark_df.write.parquet(caminho_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d2c9e6-d442-4b56-94a8-82b7ad8fb955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Especifica o caminho onde  deseja salvar o arquivo Parquet\n",
    "caminho_parquet = \"/dbfs/FileStore/mensalizada.parquet\"\n",
    "\n",
    "# Salva o DataFrame como um arquivo Parquet\n",
    "#spark_df.write.mode(\"overwrite\").parquet(caminho_parquet)\n",
    "\n",
    "# Verifica se o arquivo existe\n",
    "#if dbutils.fs.rm(caminho_parquet, True):\n",
    "#    print(f\"Arquivo {caminho_parquet} foi excluído com sucesso!\")\n",
    "#else:\n",
    "#    print(f\"O arquivo {caminho_parquet} não existe ou não pode ser excluído.\")\n",
    "\n",
    "# Escreve o novo arquivo Parquet\n",
    "#spark_df.write.parquet(caminho_parquet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fbd1fd-e688-4562-9f4b-9ba0bf99fad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|      ds_idade_cat|ds_tempo_empresa_cat|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Registra o DataFrame como uma tabela temporária\n",
    "spark_df.createOrReplaceTempView(\"mensalizada\")\n",
    "\n",
    "# Executar consultas SQL na tabela temporária\n",
    "mensalizada = spark.sql(\"SELECT * FROM mensalizada\")\n",
    "\n",
    "# Mostra o resultado da consulta\n",
    "mensalizada.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81ec66c-4ec3-4ab1-a25e-8b241bb04ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Agrupamento de dados por mês referência e cargo e contagens específicas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c94fd4-defb-417b-aaa8-60d78ca34389",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agrupamento de dados por mês referência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1a4dc0-7118-41b4-aaa8-c56e0190ae4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+---------------+\n|mes_referencia|               cargo|total_registros|\n+--------------+--------------------+---------------+\n|    2022-08-31|Anl Serv Suprimen...|             13|\n|    2022-08-31|Anl Desenvolv Sis...|             33|\n|    2022-08-31|Anl Serv Contabil Ii|              9|\n|    2022-04-30|    Instrutor Tec Ii|              2|\n|    2022-01-31| Sup Distribuicao Ii|             13|\n|    2022-10-31|Eletr Distribuica...|             62|\n|    2022-11-30|   Eletr Inspecao Ii|              6|\n|    2022-07-31|Anl Telecomunicac...|              1|\n|    2022-11-30|         Anl Teste I|              4|\n|    2022-04-30|    Anl Comercial Ii|              2|\n|    2022-09-30|            Anl Rh I|              2|\n|    2022-03-31|Ger Compras Servicos|              1|\n|    2022-07-31|   Anl Financeiro Ii|              2|\n|    2022-04-30|      Anl Financas I|              2|\n|    2022-06-30|Anl Automacao Tel...|              1|\n|    2022-11-30|       Aux Comercial|             74|\n|    2022-12-31|       Aux Comercial|             73|\n|    2022-08-31|           Tec Ti Ii|              5|\n|    2022-03-31|  Assist Logistica I|              5|\n|    2022-12-31|Anl Gestao Desemp...|              3|\n+--------------+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agrupamento dos dados por mes_referencia e cargo e contagem de registros em cada grupo\n",
    "spark_df_group = spark_df.groupBy(\"mes_referencia\", \"cargo\").agg(count(\"*\").alias(\"total_registros\"))\n",
    "\n",
    "\n",
    "# Visualização do DataFrame resultante\n",
    "spark_df_group.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776c5368-5afd-4239-9b9a-625952f52821",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Contagem do número de mulheres e homens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add8643a-b0ae-4bb2-a800-412009ebfa37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "  - Verificação de valores da coluna \"sexo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d5036f-653d-49bc-9e75-baaa0a66fc38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     sexo|\n+---------+\n|      Fem|\n|     Masc|\n| Feminino|\n|Masculino|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificação dos valores únicos na coluna 'sexo'\n",
    "valores_sexos = spark_df.select('sexo').distinct()\n",
    "\n",
    "# Visualização do resultado\n",
    "valores_sexos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50b9cd1-1a82-480c-8c98-64588b82e157",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Ajuste de dados da coluna: Masc -> Masculino e Fem -> Feminino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c985c974-1981-4331-8435-87a98b0eb45c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|matricula|mes_referencia|   funcionario|data_admissao|nacionalidade|data_demissao|estado_civil|     sexo|  empresa|data_nascimento|     grupo_cargo|               cargo|idade|tempo_empresa|      ds_idade_cat|ds_tempo_empresa_cat|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\n|       27|    2022-02-28|Colaborador 27|   2005-12-05|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1977-01-01|         Tecnico|Eletr Lv Transmis...|   45|          194|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-01-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          225|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-02-28|Colaborador 28|   2003-04-14|       Brasil|          Nan|      Casado| Feminino|Empresa 5|     1979-04-01|     Coordenador|    Coord Tesouraria|   42|          226|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-03-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          227|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-04-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          228|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-05-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          229|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-06-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          230|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-07-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          231|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-08-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          232|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-09-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          233|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-10-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          234|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-11-30|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          235|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       28|    2022-12-31|Colaborador 28|   2003-04-14|       Brasil|          Nan|    Solteiro| Feminino|Empresa 2|     1979-04-01|     Coordenador|    Coord Tesouraria|   43|          236|Entre 41 E 50 Anos|    Acima De 10 Anos|\n|       31|    2022-02-28|Colaborador 31|   2005-12-19|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1987-03-04|Supervisor/lider|Sup Operacao Usina I|   35|          194|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       34|    2022-02-28|Colaborador 34|   2006-01-03|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-04-03|     Operacional|            Soldador|   37|          193|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       44|    2022-02-28|Colaborador 44|   2006-03-20|       Brasil|          Nan|      Casado|Masculino|Empresa 6|     1984-08-28|Supervisor/lider|Sup Projetos Obra...|   37|          191|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-01-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          163|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-02-28|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 5|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          164|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-03-31|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          165|Entre 31 E 40 Anos|    Acima De 10 Anos|\n|       53|    2022-04-30|Colaborador 53|   2008-06-10|       Brasil|          Nan|      Casado|Masculino|Empresa 2|     1982-08-06|         Gerente|Ger Compras Mater...|   39|          166|Entre 31 E 40 Anos|    Acima De 10 Anos|\n+---------+--------------+--------------+-------------+-------------+-------------+------------+---------+---------+---------------+----------------+--------------------+-----+-------------+------------------+--------------------+\nonly showing top 20 rows\n\n+--------------+--------------------+---------------+\n|mes_referencia|               cargo|total_registros|\n+--------------+--------------------+---------------+\n|    2022-08-31|Ger Compras Mater...|              1|\n|    2022-08-31|Anl Desenvolv Sis...|             33|\n|    2022-08-31|Anl Serv Suprimen...|             13|\n|    2022-01-31| Sup Distribuicao Ii|             13|\n|    2022-10-31|Eletr Distribuica...|             62|\n|    2022-08-31|Anl Serv Contabil Ii|              9|\n|    2022-07-31|   Anl Financeiro Ii|              2|\n|    2022-12-31|      Anl Serv Rh Ii|              3|\n|    2022-12-31|Ger Plan Suprimentos|              1|\n|    2022-11-30|         Anl Teste I|              4|\n|    2022-10-31|       Anl Teste Iii|              2|\n|    2022-04-30|Anl Gestao Desemp...|              3|\n|    2022-11-30|       Aux Comercial|             74|\n|    2022-12-31|       Aux Comercial|             73|\n|    2022-10-31|        Coord Frotas|              4|\n|    2022-05-31|          Ger Frotas|              1|\n|    2022-08-31|           Tec Ti Ii|              5|\n|    2022-10-31|Anl Infraestrutur...|              3|\n|    2022-02-28|        Esp Contabil|              3|\n|    2022-10-31|          Anl Rh  Ii|              1|\n+--------------+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Padronização dos valores na coluna 'sexo'\n",
    "spark_df = spark_df.withColumn('sexo', \n",
    "                               when((spark_df.sexo == 'Fem') | (spark_df.sexo == 'Feminino'), 'Feminino')\n",
    "                               .when((spark_df.sexo == 'Masc') | (spark_df.sexo == 'Masculino'), 'Masculino')\n",
    "                               .otherwise('Outros'))\n",
    "\n",
    "\n",
    "spark_df.show()\n",
    "spark_df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f46dbe1-e752-4bf4-959f-87c237813ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Contagem das quantidades e visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673877a8-b64b-4dd9-bb09-322ffe862cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+---------+-----+\n|mes_referencia|               cargo|     sexo|count|\n+--------------+--------------------+---------+-----+\n|    2022-09-30|   Coord Servicos Rh|Masculino|    1|\n|    2022-09-30|       Anl Compras I|Masculino|    1|\n|    2022-12-31|Tec Monitor Operacao|Masculino|    4|\n|    2022-07-31|   Eletr Inspecao Ii|Masculino|    6|\n|    2022-03-31|Eletr Distribuica...|Masculino|   54|\n|    2022-07-31|    Anl Projetos Iii| Feminino|    1|\n|    2022-02-28|Mecanico Manutenc...|Masculino|    2|\n|    2022-06-30| Anl Banco Dados Iii|Masculino|    6|\n|    2022-08-31|Sup Distribuicao Iii|Masculino|    4|\n|    2022-09-30|          Secretaria| Feminino|    4|\n|    2022-12-31|     Ger Suprimentos|Masculino|    1|\n|    2022-06-30| Tec Transmissao Iii|Masculino|    5|\n|    2022-05-31| Anl Faturamento Iii| Feminino|    2|\n|    2022-06-30|Esp Proc Anl De N...|Masculino|    9|\n|    2022-12-31|      Ger Patrimonio|Masculino|    1|\n|    2022-01-31|        Advogado Iii|Masculino|    1|\n|    2022-02-28|    Anl Comercial Ii|Masculino|    1|\n|    2022-10-31|     Esp Banco Dados|Masculino|    2|\n|    2022-11-30|        Assist Ti Ii|Masculino|   13|\n|    2022-03-31|Coord Gestao Dese...|Masculino|    1|\n+--------------+--------------------+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agrupamento dos dados por 'mes_referencia', 'cargo' e 'sexo' e contagem da quantidade de ocorrências\n",
    "spark_df_group= spark_df.groupBy('mes_referencia', 'cargo', 'sexo').count()\n",
    "\n",
    "# Visualização do resultado da contagem\n",
    "spark_df_group.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e76441-ba51-4b96-87f6-e4f2003a01c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Contagem do número de estado civil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144c3e74-09a4-4bce-9b4e-d393f2817265",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Verificação de valores da coluna \"estado_civil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec8d348-5e19-4c62-b3e9-626d0e47781c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|  estado_civil|\n+--------------+\n|    Divorciado|\n|      Solteiro|\n| Uniao Estavel|\n|        Casado|\n|Nao preenchido|\n|         Viuvo|\n|      Separado|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verificação dos valores únicos na coluna 'estado_civil'\n",
    "valores_estado_civil = spark_df.select('estado_civil').distinct()\n",
    "\n",
    "# Visualização do resultado\n",
    "valores_estado_civil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20b42eb-f56c-493f-a79b-1228d8260d32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Contagem das quantidades e visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ef119a-2a69-4efd-8df3-7cee2f807b0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------+-----+\n|mes_referencia|               cargo|  estado_civil|count|\n+--------------+--------------------+--------------+-----+\n|    2022-12-31| Sup Distribuicao Ii|        Casado|    5|\n|    2022-07-31|Coord Processos A...|        Casado|    2|\n|    2022-02-28|        Ger Juridico|        Casado|    1|\n|    2022-11-30|Assist Administra...|      Solteiro|   23|\n|    2022-06-30|Assist Administra...|      Solteiro|   40|\n|    2022-07-31|Anl Infraestrutur...|      Solteiro|    3|\n|    2022-01-31|Anl Seg Informaca...|    Divorciado|    1|\n|    2022-01-31|        Assist Ti Ii|      Solteiro|   32|\n|    2022-10-31|        Esp Contabil|      Solteiro|    2|\n|    2022-10-31|          Anl Rh  Ii|Nao preenchido|    1|\n|    2022-12-31|   Esp Serv Contabil| Uniao Estavel|    1|\n|    2022-02-28|    Assist Tecnico I|      Solteiro|    1|\n|    2022-06-30|Ger Corp Rec Fina...|      Solteiro|    1|\n|    2022-08-31|Ger Compras Mater...|        Casado|    1|\n|    2022-05-31|Coord Contabilida...|        Casado|    1|\n|    2022-05-31|     Anl Projetos Ii|        Casado|    1|\n|    2022-11-30|Anl Desenvolv Sis...|        Casado|    7|\n|    2022-05-31|   Anl Suporte Ti Ii|      Solteiro|    5|\n|    2022-07-31|      Anl Etl Olap I|      Solteiro|    2|\n|    2022-02-28|Administrador Tec...|        Casado|    1|\n+--------------+--------------------+--------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Agrupamento dos dados por 'mes_referencia', 'cargo' e 'estado_civil' e contagem da quantidade de ocorrências\n",
    "spark_df_group = spark_df.groupBy('mes_referencia', 'cargo', 'estado_civil').count()\n",
    "\n",
    "# Visualizaçao do resultado da contagem\n",
    "spark_df_group.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969006b9-cb70-4adf-b418-43bd7ae9dc8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Visualização da Média das idades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1581f1-16df-4d0d-a480-6942e9fae8dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n|mes_referencia|               cargo|media_idade|\n+--------------+--------------------+-----------+\n|    2022-08-31|Anl Serv Suprimen...|    36.3077|\n|    2022-08-31|Anl Desenvolv Sis...|    38.5455|\n|    2022-08-31|Anl Serv Contabil Ii|    38.5556|\n|    2022-04-30|    Instrutor Tec Ii|       44.0|\n|    2022-01-31| Sup Distribuicao Ii|    43.2308|\n|    2022-10-31|Eletr Distribuica...|    38.9194|\n|    2022-11-30|   Eletr Inspecao Ii|    39.3333|\n|    2022-07-31|Anl Telecomunicac...|       59.0|\n|    2022-11-30|         Anl Teste I|      27.25|\n|    2022-04-30|    Anl Comercial Ii|       34.0|\n|    2022-09-30|            Anl Rh I|       30.0|\n|    2022-03-31|Ger Compras Servicos|       47.0|\n|    2022-07-31|   Anl Financeiro Ii|       37.5|\n|    2022-04-30|      Anl Financas I|       26.5|\n|    2022-06-30|Anl Automacao Tel...|       33.0|\n|    2022-11-30|       Aux Comercial|     31.527|\n|    2022-12-31|       Aux Comercial|    31.5753|\n|    2022-08-31|           Tec Ti Ii|       31.6|\n|    2022-03-31|  Assist Logistica I|       32.8|\n|    2022-12-31|Anl Gestao Desemp...|       33.0|\n+--------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "# Agrupamento pelos campos mes_referencia e cargo e calculo da média das idades\n",
    "spark_df_group= spark_df.groupBy(\"mes_referencia\", \"cargo\").agg(round(avg(\"idade\"), 4).alias(\"media_idade\"))\n",
    "\n",
    "# Visualização do resultado\n",
    "spark_df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750a973e-9ddd-4482-bf08-53df69152461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Visualização da Idade máxima e mínima;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa26493f-db37-40b8-a4da-be04b7f0dcd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------------+------------+\n|mes_referencia|               cargo|idade_maxima|idade_minima|\n+--------------+--------------------+------------+------------+\n|    2022-08-31|Ger Compras Mater...|          40|          40|\n|    2022-08-31|Anl Desenvolv Sis...|          56|          28|\n|    2022-08-31|Anl Serv Suprimen...|          42|          28|\n|    2022-01-31| Sup Distribuicao Ii|          56|          29|\n|    2022-10-31|Eletr Distribuica...|          48|          31|\n|    2022-08-31|Anl Serv Contabil Ii|          53|          28|\n|    2022-07-31|   Anl Financeiro Ii|          39|          36|\n|    2022-12-31|      Anl Serv Rh Ii|          36|          27|\n|    2022-12-31|Ger Plan Suprimentos|          40|          40|\n|    2022-11-30|         Anl Teste I|          33|          23|\n|    2022-10-31|       Anl Teste Iii|          44|          39|\n|    2022-04-30|Anl Gestao Desemp...|          35|          31|\n|    2022-11-30|       Aux Comercial|          55|          20|\n|    2022-12-31|       Aux Comercial|          56|          20|\n|    2022-10-31|        Coord Frotas|          42|          33|\n|    2022-05-31|          Ger Frotas|          45|          45|\n|    2022-08-31|           Tec Ti Ii|          41|          24|\n|    2022-10-31|Anl Infraestrutur...|          35|          28|\n|    2022-02-28|        Esp Contabil|          50|          39|\n|    2022-10-31|          Anl Rh  Ii|          39|          39|\n+--------------+--------------------+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Agrupamento pelos campos mes_referencia e cargo, calculo da idade máxima e mínima\n",
    "spark_df_group = spark_df.groupBy(\"mes_referencia\", \"cargo\").agg(\n",
    "    max(\"idade\").alias(\"idade_maxima\"), \n",
    "    min(\"idade\").alias(\"idade_minima\"))\n",
    "\n",
    "# Visualização do resultado\n",
    "spark_df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cebbf2-43d1-4053-b72f-9900e53ee6c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Visualização da admissão mais antiga por cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6909bd04-99d7-4b91-b62a-6c274c12dfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------------------+\n|mes_referencia|               cargo|data_admissao_mais_antiga|\n+--------------+--------------------+-------------------------+\n|    2022-08-31|Anl Serv Suprimen...|               2011-09-05|\n|    2022-08-31|Anl Desenvolv Sis...|               2006-08-21|\n|    2022-08-31|Anl Serv Contabil Ii|               2011-02-01|\n|    2022-04-30|    Instrutor Tec Ii|               1997-08-06|\n|    2022-01-31| Sup Distribuicao Ii|               1989-11-20|\n|    2022-10-31|Eletr Distribuica...|               2005-01-03|\n|    2022-11-30|   Eletr Inspecao Ii|               2008-11-03|\n|    2022-07-31|Anl Telecomunicac...|               1984-01-02|\n|    2022-11-30|         Anl Teste I|               2013-10-10|\n|    2022-04-30|    Anl Comercial Ii|               2010-04-15|\n|    2022-09-30|            Anl Rh I|               2013-12-11|\n|    2022-03-31|Ger Compras Servicos|               2019-04-09|\n|    2022-07-31|   Anl Financeiro Ii|               2008-03-03|\n|    2022-04-30|      Anl Financas I|               2015-10-01|\n|    2022-06-30|Anl Automacao Tel...|               2017-08-22|\n|    2022-11-30|       Aux Comercial|               1992-10-01|\n|    2022-12-31|       Aux Comercial|               1992-10-01|\n|    2022-08-31|           Tec Ti Ii|               2019-02-12|\n|    2022-03-31|  Assist Logistica I|               2015-04-01|\n|    2022-12-31|Anl Gestao Desemp...|               2007-12-03|\n+--------------+--------------------+-------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Agrupamento pelos campos mes_referencia e cargo, calculo da data de admissão mais antiga\n",
    "spark_df_group = spark_df.groupBy(\"mes_referencia\", \"cargo\").agg(\n",
    "    min(\"data_admissao\").alias(\"data_admissao_mais_antiga\"))\n",
    "\n",
    "# Visualização do resultado\n",
    "spark_df_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f9349e-4ff7-4be0-9d75-f71882a52312",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Após os ajustes e análises, segue uma panorama do estado do DataFrame original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba9925-de62-493d-a448-2b540d7159cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('matricula', 'bigint'), ('mes_referencia', 'date'), ('funcionario', 'string'), ('data_admissao', 'date'), ('nacionalidade', 'string'), ('data_demissao', 'string'), ('estado_civil', 'string'), ('sexo', 'string'), ('empresa', 'string'), ('data_nascimento', 'date'), ('grupo_cargo', 'string'), ('cargo', 'string'), ('idade', 'bigint'), ('tempo_empresa', 'bigint'), ('ds_idade_cat', 'string'), ('ds_tempo_empresa_cat', 'string')]\nNúmero de linhas: 47351\nNúmero de colunas: 16\n"
     ]
    }
   ],
   "source": [
    "# Visualização do esquema do DataFrame\n",
    "print(spark_df.dtypes)\n",
    "\n",
    "# Número de linhas\n",
    "num_linhas = spark_df.count()\n",
    "\n",
    "# Número de colunas\n",
    "num_colunas = len(spark_df.columns)\n",
    "\n",
    "# Exibição do número de linhas e colunas\n",
    "print(f\"Número de linhas: {num_linhas}\")\n",
    "print(f\"Número de colunas: {num_colunas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97ac3d5-fc22-4472-bee0-e566859c3c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "É possível notar a adição de mais 5 colunas e manutenção da quantidade de linhas\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2137755641172081,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Teste_Tecnico_MINEHR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
